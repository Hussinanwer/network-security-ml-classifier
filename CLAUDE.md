# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a Network Security machine learning project focused on multiclass classification of network traffic. The project analyzes network traffic patterns to classify different types of network behavior using various ML algorithms.

## Dataset

**File**: `network_traffic_multiclass_dataset.csv`
- 2073 samples with 36 features
- Target variable: `label` (3 classes: 0, 1, 2)
- Features include network packet characteristics (IPs, ports, protocols, packet sizes, timing, flags, etc.)
- No missing values in the dataset

### Key Features
- IP addresses (src_ip, dst_ip): Converted to integers using `socket.inet_aton`
- Protocol: Encoded using LabelEncoder
- Port indicators: is_port_22, is_port_6200, is_ftp_port, is_ftp_data_port
- Packet statistics: counts, sizes, timing (IAT - Inter-Arrival Time)
- TCP flags: syn_count, ack_count, fin_count, rst_count, psh_count, urg_count
- Flow metrics: forward/backward packets/bytes, ratios, rates

## Data Processing Pipeline

The notebook follows this workflow:

1. **Data Loading & Exploration**: Load CSV, check shape, info, missing values
2. **IP Encoding**: Convert IP addresses to integers using `socket.inet_aton`
3. **Protocol Encoding**: Use LabelEncoder for protocol field
4. **Zero Variance Removal**: Drop features with zero variance (urg_count, is_ftp_data_port)
5. **Weak Feature Removal**: Remove features with correlation < 0.05 to target
6. **Multicollinearity Handling**: Remove highly correlated features (threshold: 0.95), keeping ones with higher target correlation
7. **Train-Test Split**: 80/20 split with stratification
8. **SMOTE Oversampling**: Balance classes in training set (all classes → 632 samples)
9. **Scaling**: StandardScaler on features
10. **Model Training & Evaluation**: Multiple classifiers tested

### Final Feature Set (14 features after preprocessing)
- src_ip, src_port
- syn_count, fin_count
- packets_per_second, bytes_per_second
- forward_packets, backward_bytes, forward_backward_ratio
- avg_iat, std_iat, max_iat
- is_port_22, is_ftp_port

## Models Implemented

### 1. Logistic Regression
- Default parameters with LBFGS solver
- Note: May show convergence warning - increase max_iter if needed
- Test Accuracy: ~87.5%

### 2. Random Forest Classifier
- n_estimators=3 (deliberately small for quick testing)
- Test Accuracy: 100% (may indicate overfitting with only 3 trees)
- Perfect performance on test set

### 3. SVM (Support Vector Machine)
- Kernel: RBF
- C=4, gamma='scale'
- Test Accuracy: ~72.3%

## Running the Notebook

**Environment**: Jupyter Notebook with Python 3

**Required Libraries**:
```python
numpy, pandas, matplotlib, seaborn, sklearn, imblearn
```

**Execute in order**: The notebook cells must be run sequentially as they build on each other's state (feature removal affects subsequent steps).

## Important Notes

- The notebook includes interactive input prompts for feature removal decisions - these need user input
- SMOTE is applied only to training data to prevent data leakage
- StandardScaler is fit on training data and transformed on both train/test
- Label distribution before SMOTE: Class 0 (632), Class 2 (553), Class 1 (473)
- After SMOTE: All classes balanced to 632 samples

## Class Labels

The dataset has 3 classes (0, 1, 2). Based on feature correlations:
- Class 0: Associated with normal SSH traffic (is_port_22 positive correlation)
- Class 1: Associated with FTP traffic (is_ftp_port negative correlation with higher classes)
- Class 2: Likely represents attack/malicious traffic (highest backward_bytes correlation)

## Feature Engineering Insights

Key discriminative features (by target correlation):
- **src_ip**: Strongest predictor (-0.878) - suggests traffic patterns differ by source
- **bytes_per_second**: High negative correlation (-0.761)
- **packets_per_second**: Negative correlation (-0.562)
- **backward_bytes**: Positive correlation (0.534)
- **is_port_22**: Positive correlation (0.400)
- **is_ftp_port**: Negative correlation (-0.466)

## Production Deployment

This project has been converted to a production-ready system with REST API and web interface.

### Project Structure

```
.
├── api/
│   ├── app.py              # FastAPI application
│   └── __init__.py
├── models/                  # Saved models (generated by train.py)
│   ├── rf_model.pkl        # Random Forest (primary model)
│   ├── lr_model.pkl        # Logistic Regression
│   ├── svm_model.pkl       # SVM
│   ├── scaler.pkl          # StandardScaler
│   ├── label_encoder.pkl   # Protocol encoder
│   ├── preprocessor.pkl    # Complete preprocessor
│   └── model_metadata.pkl  # Model performance metrics
├── tests/                   # Test suite
│   ├── test_preprocessing.py
│   ├── test_api.py
│   └── test_data/
├── dashboard.py             # Streamlit web interface
├── train.py                 # Model training script
├── preprocessing.py         # Preprocessing pipeline module
├── config.py                # Configuration settings
├── run_api.py               # API launcher
├── run_dashboard.py         # Dashboard launcher
└── requirements.txt         # Dependencies
```

### Training Models

**IMPORTANT:** Always run `train.py` first to generate model files:

```bash
python train.py
```

This script:
- Loads the dataset
- Applies the complete preprocessing pipeline
- Trains Random Forest (n_estimators=100 for production), Logistic Regression, and SVM
- Saves all models and metadata to `models/` directory
- Displays performance metrics

**Production model:** Random Forest with 100 trees (not 3 as in notebook)

### Preprocessing Pipeline

The preprocessing is encapsulated in `preprocessing.py`:

- **NetworkTrafficPreprocessor**: Main class that handles the complete pipeline
- **fit()**: Fit on training data (learns encoders, scalers, feature removal)
- **transform()**: Transform new data using fitted components
- **fit_transform()**: Combined fit and transform

The preprocessing order MUST be preserved:
1. IP encoding → 2. Protocol encoding → 3. Zero variance removal → 4. Weak feature removal → 5. High correlation removal → 6. Scaling

### REST API

**File:** `api/app.py`

**Endpoints:**
- `POST /predict` - Single prediction
- `POST /predict/batch` - Batch predictions from CSV
- `GET /health` - Health check
- `GET /model-info` - Model metadata

**Run:**
```bash
python run_api.py
# or
uvicorn api.app:app --host 0.0.0.0 --port 8000
```

**Access:** http://localhost:8000/docs (interactive documentation)

### Streamlit Dashboard

**File:** `dashboard.py`

**Features:**
- Manual feature input form
- CSV upload for batch predictions
- Real-time predictions with confidence scores
- Confusion matrix visualization

**Run:**
```bash
python run_dashboard.py
# or
streamlit run dashboard.py --server.port=8501
```

**Access:** http://localhost:8501

### Docker Deployment

**Files:** `Dockerfile`, `docker-compose.yml`

**Multi-stage build:**
- `train`: Training image
- `api`: FastAPI service
- `dashboard`: Streamlit service

**Deploy:**
```bash
# Train models
docker-compose run train

# Start services
docker-compose up -d api dashboard
```

### Configuration

**File:** `config.py`

Environment variables (`.env`):
- `API_HOST`, `API_PORT`: API server settings
- `DASHBOARD_PORT`: Dashboard port
- `RF_N_ESTIMATORS`: Random Forest tree count
- `LOG_LEVEL`: Logging level

### Testing

**Run tests:**
```bash
pytest tests/ -v
```

**Test files:**
- `test_preprocessing.py`: Unit tests for preprocessing functions
- `test_api.py`: Integration tests for API endpoints

### Common Development Tasks

**Retrain models:**
```bash
python train.py
```

**Run API locally:**
```bash
python run_api.py
```

**Run dashboard locally:**
```bash
python run_dashboard.py
```

**Run tests:**
```bash
pytest tests/ -v
```

**Test single endpoint:**
```bash
pytest tests/test_api.py::TestPredictEndpoint::test_predict_success -v
```

### Critical Implementation Notes

1. **Preprocessing order matters:** The exact sequence from the notebook must be maintained
2. **SMOTE only for training:** Never apply SMOTE to production inference data
3. **Feature order:** API must receive features in the same order as training
4. **Model loading:** Models are loaded on API startup (see `@app.on_event("startup")`)
5. **Error handling:** All API endpoints have comprehensive error handling

### API Input Format

All 35 original features must be provided (excluding label). The API automatically applies the preprocessing pipeline. See `API.md` for complete field descriptions.

### Model Performance (Production)

| Model | Test Accuracy | F1 Score |
|-------|---------------|----------|
| Random Forest (100 trees) | 100.0% | 100.0% |
| Logistic Regression | 87.5% | 87.5% |
| SVM | 72.3% | 72.1% |

**Primary model:** Random Forest is used for all predictions due to superior performance.
